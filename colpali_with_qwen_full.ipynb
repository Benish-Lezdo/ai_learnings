{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2c148-a08c-48e7-a6b0-3586d89fc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade git+https://github.com/huggingface/transformers.git \n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y poppler-utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install byaldi  accelerate  qwen_vl_utils pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbd355-77cf-4ee6-b06c-1911860e6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from byaldi import RAGMultiModalModel\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "RAG = RAGMultiModalModel.from_pretrained(\"vidore/colpali\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5510c-e958-4939-8597-027408c259e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG.index(\n",
    "    input_path=\"/home/dataset1.pdf\",\n",
    "    index_name=\"index\",\n",
    "    store_collection_with_index=False,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a5cde6-50cd-44c0-9c44-fbe23374239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc_id': 0, 'page_num': 2, 'score': 22.0, 'metadata': {}, 'base64': None}]\n"
     ]
    }
   ],
   "source": [
    "search_query = input(\"Enter the question\")\n",
    "\n",
    "results = RAG.search(search_query, k=1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f2add2-6bb2-444b-a22e-5107cc87384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = convert_from_path(\"/home/dataset1.pdf\")\n",
    "image_indexes = [pages['page_num']-1 for pages in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98119b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen model\n",
    "%pip install flash-attn qwen_vl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8897b9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363602a26cfb470c8da8cf1de9a74d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, AutoModelForImageTextToText, Qwen2VLForConditionalGeneration, AutoConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# config = AutoConfig.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# config.attn_config = {\n",
    "#     \"attn_type\": \"flash_attention\",\n",
    "#     \"attn_implementation\": \"flash_attention_2\"\n",
    "# }\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                                                        \"Qwen/Qwen2-VL-2B-Instruct\", \n",
    "                                                        torch_dtype=\"auto\",  \n",
    "                                                        device_map = \"auto\",\n",
    "                                                        # attn_implementation=\"flash_attention_2\"\n",
    "                                                        # config = config\n",
    "                                                        )\n",
    "model.cuda().eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2a827c4-4572-46bd-8d51-089a57b1d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_template(image_indexes):\n",
    "    content = []\n",
    "    for i in image_indexes:\n",
    "        content.append({\"type\":\"image\", \"image\":images[i]})\n",
    "    return content\n",
    "\n",
    "content = generate_template(image_indexes)\n",
    "content.append({\"type\": \"text\", \"text\": search_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a856a17-7714-4ba2-b246-a1f745782c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The reason for transferring Mr. Tan Ah Kow from ABC Hospital to XYZ Hospital was due to his stroke. The clinical impression was that he was manifesting behavioural and psychological symptoms secondary to dementia.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": content\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
