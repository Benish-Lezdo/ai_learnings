{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Used llama 3 8B-instruct\n",
    "* Good at reading checked forms\n",
    "* Loading takes much time in A6000 GPU(nearly 30 mins)\n",
    "* Can process one image at a time\n",
    "* Can not able to handle complex handwritten texts \n",
    "* Produces Wrong answer if the answer is not in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers einops xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f38a830e54b45b39c91dacdd635f4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e973245b8638485186936d54e1bd1640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:  95%|#########4| 4.75G/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7b8756841d4e96b364f5eb066ea348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ec0b32f78a476294f4872a1ba349ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3a5f70035f40e09061672295b5c87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1c9acd1a3d416ab2d52d57d08f1e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cfe082ad11460cbefa61bab6b9678e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"THUDM/cogvlm2-llama3-chat-19B\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TORCH_TYPE = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=TORCH_TYPE,\n",
    "    trust_remote_code=True,\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant provides helpful, detailed, and polite answers to the user's questions based on the image. If the assistant cannot find the accurate answer from the image, the assistant responds with 'Answer Not Found.' USER: {} ASSISTANT:\"\n",
    "\n",
    "image_path1 = '/home/redacted_pdf_pg_no_69.png'\n",
    "image_path2 = '/home/redacted_pdf_pg_no_28.png'\n",
    "image_path3 = \"/home/Screenshot 2024-11-15 124654.png\"\n",
    "image_path4 = \"/home/Screenshot 2024-11-18 164554.png\"\n",
    "image_path5 = \"/home/Screenshot 2024-11-18 165115.png\"\n",
    "\n",
    "image1 = Image.open(image_path1).convert('RGB')\n",
    "image2 = Image.open(image_path2).convert('RGB')\n",
    "image3 = Image.open(image_path3).convert('RGB')\n",
    "image4 = Image.open(image_path4).convert('RGB')\n",
    "image5 = Image.open(image_path5).convert(\"RGB\")\n",
    "history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Human:\")\n",
    "\n",
    "\n",
    "input_by_model = model.build_conversation_input_ids(\n",
    "        tokenizer,\n",
    "        query=query,\n",
    "        history=history,\n",
    "        images=[image5],\n",
    "        template_version='chat'\n",
    "    )\n",
    "inputs = {\n",
    "    'input_ids': input_by_model['input_ids'].unsqueeze(0).to(DEVICE),\n",
    "    'token_type_ids': input_by_model['token_type_ids'].unsqueeze(0).to(DEVICE),\n",
    "    'attention_mask': input_by_model['attention_mask'].unsqueeze(0).to(DEVICE),\n",
    "    'images': [[input_by_model['images'][0].to(DEVICE).to(TORCH_TYPE)]] \n",
    "}\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"pad_token_id\": 128002,  \n",
    "}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    response = response.split(\"<|end_of_text|>\")[0]\n",
    "    print(\"\\nCogVLM2:\", response)\n",
    "history.append((query, response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
